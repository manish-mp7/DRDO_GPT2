# -*- coding: utf-8 -*-
"""DRDO_GPT2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U5VhxaBvvG2MdHBrRdZuSaEUCnru-0j6
"""

from google.colab import drive
drive.mount("/content/gdrive", force_remount=True)

from google.colab import files
uploaded = files.upload()

import nltk
import re
import numpy
import pandas as pd
from gensim import models

nltk.download('punkt')
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

file_name = "doc1.txt"
text = uploaded[file_name].decode("utf-8")

text = '''
Sorting involves rearranging information into either ascending or descending order. There are many
sorting algorithms, among which is Bubble Sort. Bubble Sort is not known to be a very good sorting
algorithm because it is beset with redundant comparisons. However, efforts have been made to
improve the performance of the algorithm. With Bidirectional Bubble Sort, the average number of
comparisons is slightly reduced and Batcher’s Sort similar to Shellsort also performs significantly
better than Bidirectional Bubble Sort by carrying out comparisons in a novel way so that no
propagation of exchanges is necessary. Bitonic Sort was also presented by Batcher and the strong
point of this sorting procedure is that it is very suitable for a hard-wired implementation using a sorting
network. This paper presents a meta algorithm called Oyelami’s Sort that combines the technique of
Bidirectional Bubble Sort with a modified diminishing increment sorting. The results from the
implementation of the algorithm compared with Batcher’s Odd-Even Sort and Batcher’s Bitonic Sort
showed that the algorithm performed better than the two in the worst case scenario. The implication is
that the algorithm is faster.
 '''

dataset = nltk.sent_tokenize(text)
for i in range(len(dataset)):
    dataset[i] = dataset[i].lower()
    dataset[i] = re.sub(r'\W', ' ', dataset[i])
    dataset[i] = re.sub(r'\s+', ' ', dataset[i]) 

stop_words = set(stopwords.words('english'))
pre_processed_sentence = []
filtered_sentence = []

for i in range(len(dataset)):
  word_tokens = word_tokenize(dataset[i])
 
  # filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]
 
  for w in word_tokens:
      if w not in stop_words:
          filtered_sentence.append(w)

word2count = {}

for word in filtered_sentence:
    if word not in word2count.keys():
        word2count[word] = 1
    else:
        word2count[word] += 1

print(word2count)

text





filtered_sentence

word2count = {}

for word in filtered_sentence:
    if word not in word2count.keys():
        word2count[word] = 1
    else:
        word2count[word] += 1



for word in word2count.keys():
  print(word)

# Commented out IPython magic to ensure Python compatibility.
# ONLY RUN ONCE
# %cd /content/gdrive/My Drive/DRDO_Project

# !git clone https://github.com/openai/gpt-2.git

# Commented out IPython magic to ensure Python compatibility.
# %cd gpt-2

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

!pip3 install -r requirements.txt

!python3 download_model.py 124M
!python3 download_model.py 355M
!python3 download_model.py 774M
!python3 download_model.py 1558M

!python3 src/interactive_conditional_samples.py --top_k 40

GPT2_model = TransformerSummarizer(transformer_type="GPT2",transformer_model_key="gpt2-medium")
full = ''.join(GPT2_model(body, min_length=60))
print(full)